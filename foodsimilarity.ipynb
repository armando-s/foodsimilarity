{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "foodsimilarity.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3taEI-FzGmYK"
      },
      "source": [
        "#Food Similarity\n",
        "\n",
        "###Author: Armando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsJi8g0WC1zV"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import  activations, datasets, layers, losses, metrics, models, optimizers, regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tIQ0GJyHvJq"
      },
      "source": [
        "Parts of the code are based on the Keras example of siamese networks, although not the model itself. \n",
        "\n",
        "The source can be found here: https://keras.io/examples/vision/siamese_network/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb_YwldFElJf"
      },
      "source": [
        "seed = 13\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80KURyohHC9S"
      },
      "source": [
        "The notebook was run in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfMJmiL6OU32"
      },
      "source": [
        "# Load dataset\n",
        "img_path = \"/content/drive/MyDrive/foodsimilarity/food/food/\"\n",
        "train_triplets_path = \"/content/drive/MyDrive/foodsimilarity/train_triplets.txt\"\n",
        "test_triplets_path = \"/content/drive/MyDrive/foodsimilarity/test_triplets.txt\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHq32GfwVMnh"
      },
      "source": [
        "Input shape depends on the selected pre-trained model we use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji9V4bDuDbV6"
      },
      "source": [
        "input_shape = (299,299,3)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXkGvZWNMeWm"
      },
      "source": [
        "To get some features embeddings for the images, a pretrained ResNet is used, in this case ResNetV2. First, features are computed for all 10000 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A6Mo8OrEoHy"
      },
      "source": [
        "def pretrained_feature(input_shape):\n",
        "    resnet = tf.keras.applications.InceptionResNetV2(pooling='avg',include_top=False)\n",
        "    # the basic features embeddings are computed by a ResNet\n",
        "    resnet.trainable = False\n",
        "\n",
        "    # input\n",
        "    x = x_in = Input(shape=input_shape)\n",
        "    x = resnet(x)\n",
        "\n",
        "    model = Model(inputs=x_in, outputs=x)\n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5QOhikGI8H0"
      },
      "source": [
        "Load all unzipped images from a folder and pass them to the ResNetV2 to get features empeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qGFrtmsE5iL"
      },
      "source": [
        "def load_images(path, batch_size=1):\n",
        "    # image indices\n",
        "    idx_images = 10000\n",
        "    idx = 0\n",
        "\n",
        "    while True:\n",
        "        batch = []\n",
        "    # we load all images and get the embeddings from a pretrained NN\n",
        "        while len(batch) < batch_size:\n",
        "            img_name= path + '{0:05}'.format(idx) + \".jpg\"\n",
        "            img = load_img(img_name)\n",
        "            img = tf.keras.applications.inception_resnet_v2.preprocess_input(img_to_array(img))\n",
        "            batch.append(img)\n",
        "            idx = (idx + 1) % idx_images\n",
        "\n",
        "        batch = np.array(batch)\n",
        "        labels = np.zeros(batch_size)\n",
        "\n",
        "        try:\n",
        "            yield batch, labels\n",
        "        except StopIteration:\n",
        "            return"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVrTi3WTUcfv"
      },
      "source": [
        "Wrapper function to load all images from food folder and get the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0M4THCTK4eN"
      },
      "source": [
        "def feature_extraction():\n",
        "    feature_extraction = pretrained_feature(input_shape)\n",
        "    images = load_images(img_path, 1)\n",
        "    feature = feature_extraction.predict(images, steps=10000)\n",
        "    return feature"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpCuoVHGXEku"
      },
      "source": [
        "Return the training dataset and labels or just the test dataset without labels. Important is that all triplets are concatenated, so only one single neural network must be trained and not a Siamese network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng6BC0SZM4gN"
      },
      "source": [
        "def get_triplets(features, triplets_file, labels_est=False):\n",
        "    train_tensors = []\n",
        "    labels = []\n",
        "\n",
        "    # read triplets\n",
        "    trips = pd.read_csv(triplets_file, delim_whitespace=True, header=None, names=[\"A\", \"P\", \"N\"])\n",
        "\n",
        "    for i in range(len(trips)):\n",
        "        triplet = trips.iloc[i]\n",
        "        A, P, N = triplet['A'], triplet['P'], triplet['N']\n",
        "        # compute features per image\n",
        "        tensor_a = features[A]\n",
        "        tensor_p = features[P]\n",
        "        tensor_n = features[N]\n",
        "        # concatenate features of all images into one numpy array\n",
        "        triplet_tensor = np.concatenate((tensor_a, tensor_p, tensor_n), axis=-1)\n",
        "        if (labels_est):\n",
        "            # this is just for the training dataset\n",
        "            reverse_triplet_tensor = np.concatenate((tensor_a, tensor_n, tensor_p), axis=-1)\n",
        "            train_tensors.append(triplet_tensor)\n",
        "            labels.append(1)\n",
        "            train_tensors.append(reverse_triplet_tensor)\n",
        "            labels.append(0)\n",
        "        else:\n",
        "            train_tensors.append(triplet_tensor)\n",
        "\n",
        "    train_tensors = np.array(train_tensors)\n",
        "    if (labels_est):\n",
        "        labels = np.array(labels)\n",
        "        return train_tensors, labels\n",
        "    else:\n",
        "        return train_tensors"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uIGugnkGVIB"
      },
      "source": [
        "Load pre-computed features or compute features if not available yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B08q2mF6FQk5"
      },
      "source": [
        "if os.path.exists(\"/content/drive/MyDrive/foodsimilarity/features.txt\"):\n",
        "  features = np.loadtxt(\"/content/drive/MyDrive/foodsimilarity/features.txt\", delimiter=\",\")\n",
        "else:\n",
        "  features = feature_extraction()\n",
        "  np.savetxt(\"/content/drive/MyDrive/foodsimilarity/features.txt\", features, delimiter=\",\", fmt='%1.10e')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlJ-nQ-nJU8f"
      },
      "source": [
        "Preprocess the train and test tensors as well as the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKP199OpNJQX"
      },
      "source": [
        "train_tensors, labels = get_triplets(features, train_triplets_path, labels_est=True)\n",
        "test_tensors = get_triplets(features, test_triplets_path, labels_est=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBYkLzxA_GmC",
        "outputId": "167f8994-87bf-4fd9-dbba-62af75013811"
      },
      "source": [
        "(test_tensors.shape, train_tensors.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((59544, 4608), (119030, 4608))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk0o0YfMU1Mu"
      },
      "source": [
        "Define a simple model that can be trained to learn the embeddings of an anchor image, a positive and a negative image. Here no pre-trained model is necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_7H8ediPJKR",
        "outputId": "c9281ad7-962c-4346-ab76-b9b129d59a7c"
      },
      "source": [
        "model = models.Sequential([\n",
        "            layers.InputLayer(train_tensors.shape[1:]),\n",
        "            layers.Dropout(0.7, name='dropout_0'),\n",
        "            layers.Dense(1152, activation=activations.relu, name='dense_1'),\n",
        "            layers.Dense(288, activation=activations.relu, name='dense_2'),\n",
        "            layers.Dense(72, activation=activations.relu, name='dense_3'),\n",
        "            layers.Dense(18, activation=activations.relu, name='dense_4'),\n",
        "            layers.Dense(1, activation=activations.sigmoid, name='output')   \n",
        "        ])\n",
        "model.compile(optimizer=optimizers.Adam(),\n",
        "                   loss=losses.binary_crossentropy,\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_0 (Dropout)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1152)              5309568   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 288)               332064    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 72)                20808     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 18)                1314      \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 1)                 19        \n",
            "=================================================================\n",
            "Total params: 5,663,773\n",
            "Trainable params: 5,663,773\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwgaF1_pRjMu",
        "outputId": "a0334ab8-8f92-47c5-a1cd-b04c2dd8944f"
      },
      "source": [
        "model.fit(x=train_tensors, y = labels, epochs=7)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "3720/3720 [==============================] - 29s 7ms/step - loss: 0.6086 - accuracy: 0.6577\n",
            "Epoch 2/7\n",
            "3720/3720 [==============================] - 28s 8ms/step - loss: 0.5785 - accuracy: 0.6886\n",
            "Epoch 3/7\n",
            "3720/3720 [==============================] - 28s 8ms/step - loss: 0.5684 - accuracy: 0.6978\n",
            "Epoch 4/7\n",
            "3720/3720 [==============================] - 28s 8ms/step - loss: 0.5609 - accuracy: 0.7031\n",
            "Epoch 5/7\n",
            "3720/3720 [==============================] - 28s 7ms/step - loss: 0.5552 - accuracy: 0.7079\n",
            "Epoch 6/7\n",
            "3720/3720 [==============================] - 28s 8ms/step - loss: 0.5499 - accuracy: 0.7140\n",
            "Epoch 7/7\n",
            "3720/3720 [==============================] - 28s 8ms/step - loss: 0.5462 - accuracy: 0.7152\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe75be04f10>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EyGT7jINci3"
      },
      "source": [
        "y_test = model.predict(test_tensors)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36kYY3GMfGeI"
      },
      "source": [
        "We return classifications so the predictions have to be in $\\{0,1\\}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFZpRGZpNeuu"
      },
      "source": [
        "y_test_thresh = np.where(y_test < 0.5, 0, 1)\n",
        "np.savetxt('/content/drive/MyDrive/foodsimilarity/result.txt', y_test_thresh, fmt='%d')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTS470NYMw7h"
      },
      "source": [
        "For this particular dataset, a Siamese network could not outperform a simple neural network with concatenated features. By swapping the order of the second and third columns in the training set to create negative examples, the network can be trained with a larger sample. "
      ]
    }
  ]
}
# foodsimilarity
Training a neural network to recognize which two dishes taste most similar to each other based on images of three dishes. 

## Task
Given are 10000 pictures of dishes. The goal is to learn which dishes are similar in taste based on the image alone. This is evaluated by comparing triplets and deciding which of the last two images is more similar in taste to the first. The labels were assigned manually by human raters. The task is not trivial for a human, as there are many cases where all three images show similar tasting dishes or all three images show completely different dishes. This must be taken into account when evaluating the performance, since one cannot expect near-perfect results in this problem.

## Data
Unfortunately the dataset cannot be made public (at least not by me), but the solution and techniques apply to many similar tasks and datasets. In total, the dataset contains 10000 images numbered from `00000.jpg` to `09999.jpg`. In addition, there are 89287 manually labeled images, of which 59515 are included in the training set that we have access to, while we cannot access the labels of the remaining 29772 images, but must predict them. The training set is provided as a .txt file with 59515 lines and 3 numbers per line. Those number are the names of the images, the first one is the anchor to which the other two are compared, the second one is the more similar in taste to the anchor and the third one is the less similar in taste compared to the anchor picture, e.g. `00523 07345 01213`, means that image number `07345` is more similar in taste to `00523` than `01213` is to `00523`. 

## Model
To obtain a feature representation for the images, a pre-trained neural network can be used. In this case, the images were fed into a ResNetV2 model and the corresponding extracted features were stored in the `features` matrix with dimensions 10000 x 1536. Instead of training a Siamese neural network where each network is fed with the anchor image and the positive or negative image to learn, here all 3 feature representations of a triplet are concatenated and fed into a single neural network with the corresponding label. When the triplets are prepared and concatenated into a training set, the order of the positive and negative images, i.e., the image number of the second and third columns in the training set, is swapped to produce a negative observation. In this way, there are 119030 observations in the training set, since each triplet is once in the correct order with label 1 and once in reverse order with label 0. This allows the entire training set to be used, rather than just swapping and using a subset of the triplets. The size of the training set is 119030 x 4608, and that of the test set is 59544 x 4608, where 4608 is due to the concatenation of feature extractions per image. The actual learning takes place in a user-defined sequential model with 4 hidden layers. 

## Results
It turns out that how the features are extracted and preprocessed and which pre-trained model is used is more important than how the sequential network is modeled. For this particular dataset, a Siamese network could not outperform a simple neural network with concatenated features.
